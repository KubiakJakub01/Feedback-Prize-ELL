experiment_params:
  track: false
  wandb_project_name: "feedback-prize-ell"
  wandb_entity: None
  device: "cuda"
  ddp: False
  log_step: 10
  save_step: 100
  valid_step: 100

model_params:
  config:
    model_name: custom_model
    model_checkpoint: bert-base-uncased
    freeze: True
    pooling: lstm
    num_layers: 2
    hidden_size: 768
    num_classes: 6
  save_path: models/
  optimizer_name: adam
  type_of_scheduler: one_cycle
  loss_fn: smooth_l1
  metrics: ["mcrmse"]

data_params:
  train_data_path: data/train.csv
  valid_data_path: data/valid.csv
  text_col: full_text
  label_cols: [
            "cohesion",
            "syntax",
            "vocabulary",
            "phraseology",
            "grammar",
            "conventions",
        ]

hyperparameters:
  epochs: 5
  batch_size: 2
  learning_rate: 1e-5
  max_length: 512
  padding: "max_length"
  num_warmup_steps: 0
  max_grad_norm: 1.0
  num_workers: 1
  truncate: true
  fp16_backend: "auto"

evaluation_params:
  data_path: data/test.csv
  evaluation_dir: evaluation
  batch_size: 2
  num_workers: 1


