experiment_params:
  track: false
  wandb_project_name: "feedback-prize-ell"
  wandb_entity: None
  ddp: False
  log_step: 10
  save_step: 100
  valid_step: 100

model_params:
  model_checkpoint: bert-base-uncased
  model_name: bert
  save_path: models/
  optimizer_name: adam
  type_of_scheduler: one_cycle
  loss_fn: cross_entropy
  
data_params:
  train_data_path: data/train.csv
  valid_data_path: data/valid.csv
  text_col: full_text
  label_cols: [
            "cohesion",
            "syntax",
            "vocabulary",
            "phraseology",
            "grammar",
            "conventions",
        ]

hyperparameters:
  epochs: 5
  batch_size: 32
  learning_rate: 1e-5
  max_length: 512
  padding: "max_length"
  num_warmup_steps: 0
  max_grad_norm: 1.0
  num_workers: 1
  truncate: true
  fp16_backend: "auto"
